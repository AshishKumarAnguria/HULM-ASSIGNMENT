{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # warning level\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # use GPU 0\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import sys\n",
    "from math import ceil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainVars(ID):\n",
    "    global num_batches_per_epoch\n",
    "    global trainX1\n",
    "    global trainX2\n",
    "    global trainYcat\n",
    "    global n_input1\n",
    "    global n_input2\n",
    "    global trainX\n",
    "    # load train variables\n",
    "    vars_path_ID = vars_path + ID + '/'\n",
    "    trainX = np.load(vars_path_ID+'trainX.npy')\n",
    "    trainYcat = np.load(vars_path_ID+'trainYcat.npy')\n",
    "    #print(trainX)\n",
    "    #df = pd.DataFrame(trainX)\n",
    "    #df\n",
    "\n",
    "    # mask some parts to make the desired signal\n",
    "    mask1 = np.zeros((780,), dtype=bool)\n",
    "    mask2 = np.zeros((780,), dtype=bool)\n",
    "    mask1[0:4]=1\n",
    "    mask1[0+390:4+390]=1\n",
    "    mask1[4:4+250+1]=1\n",
    "    mask1[4+390:4+390+250+1]=1\n",
    "    mask2[0:4]=1\n",
    "    mask2[0+390:4+390]=1\n",
    "    mask2[4+250+1:4+250+1+135]=1\n",
    "    mask2[4+250]=1\n",
    "    mask2[4+390+250+1:4+390+250+1+135]=1\n",
    "    mask2[4+390+250]=1\n",
    "    trainX1 = trainX[:,:,mask1]\n",
    "    trainX2 = trainX[:,:,mask2]\n",
    "\n",
    "    # make some general numbers\n",
    "    num_examples = trainX.shape[0]\n",
    "    num_batches_per_epoch = ceil(num_examples/batch_size)\n",
    "    #print('trX1->',trainX1.shape)\n",
    "    #print('trX2->',trainX2.shape)\n",
    "    n_input1 = (trainX1.shape[2]//n_steps1)\n",
    "\n",
    "    n_input2 = (trainX2.shape[2]//n_steps2)\n",
    "    trainX1 = trainX1.reshape((trainX1.shape[0], n_steps1, n_input1))\n",
    "    trainX2 = trainX2.reshape((trainX2.shape[0], n_steps2, n_input2))\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_path = '../Data/AA/'\n",
    "models_path = '../models/'\n",
    "logs_path = './Logs/'\n",
    "n_classes = 7\n",
    "n_steps = 10\n",
    "n_hidden = 30\n",
    "num_epochs = 100\n",
    "num_layers = 1\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "ret = 0\n",
    "n_steps1 = n_steps\n",
    "n_steps2 = n_steps\n",
    "n_hidden1 = n_hidden\n",
    "n_hidden2 = n_hidden\n",
    "#IDs = ['100','101','103','105','106','108','109','111','112','113','114','115','116','117','118','119','121','122','123','124','200', '201', '202', '203', '205', '207', '208', '209', '210', '212', '213', '214', '215', '219', '220', '221', '222', '223', '228', '230', '231', '232', '233', '234'] # all records\n",
    "IDs = ['100','101','103','105','106','108','109','200', '201', '202', '203', '205', '207', '208', '209', '210', '212', '213', '214', '215'] # all records\n",
    "#IDs = ['200', '201', '202', '203', '205', '207', '208', '209', '210', '212', '213', '214', '215', '219', '220', '221', '222', '223', '228', '230', '231', '232', '233', '234']\n",
    "#runs = np.random.permutation(np.arange(int(sys.argv[1]), int(sys.argv[2])))#ret#['_1', '_2', '_3', ..., '_50']\n",
    "runs = np.random.permutation(np.arange(int(1), int(10)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is architecture of model-alpha. x1 and x2 are passed to separate LSTM unit and their output is cascaded and passed to FC neural network which will give final prediction.\n",
    "<img src=\"modela.gif\" , width=100, height=200>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph():\n",
    "    global x1\n",
    "    global x2\n",
    "    global y\n",
    "    global weights\n",
    "    global biases\n",
    "    global cost\n",
    "    global optimizer\n",
    "    global correct_pred\n",
    "    global accuracy\n",
    "    global pred\n",
    "    global init\n",
    "    global saver\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "\n",
    "        # reset Graph\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # tf Graph input\n",
    "        x1 = tf.placeholder(\"float\", [None, n_steps1, n_input1], name='x1')\n",
    "        x2 = tf.placeholder(\"float\", [None, n_steps2, n_input2], name='x2')\n",
    "        y = tf.placeholder(\"float\", [None, n_classes], name='y')\n",
    "\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, n_steps, n_input)\n",
    "        # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "        # Permuting batch_size and n_steps\n",
    "        x11 = tf.transpose(x1, [1, 0, 2])\n",
    "        x22 = tf.transpose(x2, [1, 0, 2])\n",
    "        # Reshaping to (n_steps*batch_size, n_input)\n",
    "        x11 = tf.reshape(x11, [-1, n_input1])\n",
    "        x22 = tf.reshape(x22, [-1, n_input2])\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "        x11 = tf.split(x11, n_steps1, 0)\n",
    "        x22 = tf.split(x22, n_steps2, 0)\n",
    "\n",
    "        # Define weights\n",
    "        weights = tf.Variable(tf.random_normal([n_hidden1+n_hidden2, n_classes]))\n",
    "        biases = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "        # Define lstm cells with tensorflow\n",
    "        lstm_cell1 = rnn.LSTMCell(n_hidden1, use_peepholes=False)\n",
    "        lstm_cell2 = rnn.LSTMCell(n_hidden2, use_peepholes=False)\n",
    "\n",
    "        # Get outputs of lstm cells\n",
    "        with tf.variable_scope('rnn1'):\n",
    "            outputs1, states1 = rnn.static_rnn(lstm_cell1, x11, dtype=tf.float32)\n",
    "        with tf.variable_scope('rnn2'):\n",
    "            outputs2, states2 = rnn.static_rnn(lstm_cell2, x22, dtype=tf.float32)\n",
    "\n",
    "        # Linear activation, using rnn inner loop last output\n",
    "        tmp1 = tf.concat(values=[outputs1[-1], outputs2[-1]], axis=1)\n",
    "        tmp1 = tf.matmul(tmp1, weights)\n",
    "        pred = tf.add(tmp1, biases, name='pred')\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # Evaluate model\n",
    "        correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='Accuracy')\n",
    "\n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # write the model\n",
    "        #writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "#         with tf.Session() as sess:\n",
    "#             writer = tf.summary.FileWriter(\"output_graph\", sess.graph)\n",
    "#             print(sess.run(pred))\n",
    "#             writer.close()\n",
    "\n",
    "        # 'Saver' op to save and restore all the variables\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(ID):\n",
    "    global sess\n",
    "    global predY\n",
    "    global trainX1\n",
    "    global trainX2\n",
    "    global trainYcat\n",
    "\n",
    "    # set configs\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True # usw memory of GPUs more efficiently\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for curr_epoch in range(num_epochs):\n",
    "            for batch in range(num_batches_per_epoch):\n",
    "\n",
    "                # Preparing required batch\n",
    "                batchRange = range(batch*batch_size, min((batch+1)*batch_size,trainX1.shape[0]))\n",
    "                batch_x1 = trainX1[batchRange,:,:]\n",
    "                batch_x2 = trainX2[batchRange,:,:]\n",
    "                batch_y = trainYcat[batchRange,:]\n",
    "\n",
    "                # Reshape data to get seq of required elements\n",
    "                batch_x1 = batch_x1.reshape((batch_x1.shape[0], n_steps1, n_input1))\n",
    "                batch_x2 = batch_x2.reshape((batch_x2.shape[0], n_steps2, n_input2))\n",
    "\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(optimizer, feed_dict={x1: batch_x1, y: batch_y, x2:batch_x2})\n",
    "\n",
    "                # Calculate accuracy of last epoch batch and batch loss\n",
    "                acc, loss = sess.run([accuracy, cost], feed_dict={x1: batch_x1, y: batch_y, x2:batch_x2})\n",
    "\n",
    "            #losses.append(loss)\n",
    "            #acces.append(acc)\n",
    "\n",
    "        # save the prediction of train datas\n",
    "        predY = sess.run(pred, feed_dict={x1: trainX1, y: trainYcat, x2:trainX2})\n",
    "        directory = os.path.dirname(predsPath)\n",
    "        try:\n",
    "            os.stat(directory)\n",
    "        except:\n",
    "            os.mkdir(directory)\n",
    "        np.save( predsPath + ID + '_predY.npy', predY)\n",
    "\n",
    "        # Save model weights to disk\n",
    "        directory = os.path.dirname(model_path)\n",
    "        try:\n",
    "            os.stat(directory)\n",
    "        except:\n",
    "            os.mkdir(directory)\n",
    "        model_path_ID = model_path + ID + '/'\n",
    "        directory = os.path.dirname(model_path_ID)\n",
    "        try:\n",
    "            os.stat(directory)\n",
    "        except:\n",
    "            os.mkdir(directory)\n",
    "        del directory\n",
    "        save_path = saver.save(sess, model_path_ID+'model.ckpt')\n",
    "        print(\"ModelA of %s was saved\" % str(ID+'_'+str(run)))\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelA of 100_2 was saved\n",
      "ModelA of 100_7 was saved\n",
      "ModelA of 100_5 was saved\n",
      "ModelA of 100_9 was saved\n",
      "ModelA of 100_6 was saved\n",
      "ModelA of 100_4 was saved\n",
      "ModelA of 100_3 was saved\n",
      "ModelA of 100_1 was saved\n",
      "ModelA of 100_8 was saved\n",
      "ModelA of 101_2 was saved\n",
      "ModelA of 101_7 was saved\n",
      "ModelA of 101_5 was saved\n",
      "ModelA of 101_9 was saved\n",
      "ModelA of 101_6 was saved\n",
      "ModelA of 101_4 was saved\n",
      "ModelA of 101_3 was saved\n",
      "ModelA of 101_1 was saved\n",
      "ModelA of 101_8 was saved\n",
      "ModelA of 103_2 was saved\n",
      "ModelA of 103_7 was saved\n",
      "ModelA of 103_5 was saved\n",
      "ModelA of 103_9 was saved\n",
      "ModelA of 103_6 was saved\n",
      "ModelA of 103_4 was saved\n",
      "ModelA of 103_3 was saved\n",
      "ModelA of 103_1 was saved\n",
      "ModelA of 103_8 was saved\n",
      "ModelA of 105_2 was saved\n",
      "ModelA of 105_7 was saved\n",
      "ModelA of 105_5 was saved\n",
      "ModelA of 105_9 was saved\n",
      "ModelA of 105_6 was saved\n",
      "ModelA of 105_4 was saved\n",
      "ModelA of 105_3 was saved\n",
      "ModelA of 105_1 was saved\n",
      "ModelA of 105_8 was saved\n",
      "ModelA of 106_2 was saved\n",
      "ModelA of 106_7 was saved\n",
      "ModelA of 106_5 was saved\n",
      "ModelA of 106_9 was saved\n",
      "ModelA of 106_6 was saved\n",
      "ModelA of 106_4 was saved\n",
      "ModelA of 106_3 was saved\n",
      "ModelA of 106_1 was saved\n",
      "ModelA of 106_8 was saved\n",
      "ModelA of 108_2 was saved\n",
      "ModelA of 108_7 was saved\n",
      "ModelA of 108_5 was saved\n",
      "ModelA of 108_9 was saved\n",
      "ModelA of 108_6 was saved\n",
      "ModelA of 108_4 was saved\n",
      "ModelA of 108_3 was saved\n",
      "ModelA of 108_1 was saved\n",
      "ModelA of 108_8 was saved\n",
      "ModelA of 109_2 was saved\n",
      "ModelA of 109_7 was saved\n",
      "ModelA of 109_5 was saved\n",
      "ModelA of 109_9 was saved\n",
      "ModelA of 109_6 was saved\n",
      "ModelA of 109_4 was saved\n",
      "ModelA of 109_3 was saved\n",
      "ModelA of 109_1 was saved\n",
      "ModelA of 109_8 was saved\n",
      "ModelA of 200_2 was saved\n",
      "ModelA of 200_7 was saved\n",
      "ModelA of 200_5 was saved\n",
      "ModelA of 200_9 was saved\n",
      "ModelA of 200_6 was saved\n",
      "ModelA of 200_4 was saved\n",
      "ModelA of 200_3 was saved\n",
      "ModelA of 200_1 was saved\n",
      "ModelA of 200_8 was saved\n",
      "ModelA of 201_2 was saved\n",
      "ModelA of 201_7 was saved\n",
      "ModelA of 201_5 was saved\n",
      "ModelA of 201_9 was saved\n",
      "ModelA of 201_6 was saved\n",
      "ModelA of 201_4 was saved\n",
      "ModelA of 201_3 was saved\n",
      "ModelA of 201_1 was saved\n",
      "ModelA of 201_8 was saved\n",
      "ModelA of 202_2 was saved\n",
      "ModelA of 202_7 was saved\n",
      "ModelA of 202_5 was saved\n",
      "ModelA of 202_9 was saved\n",
      "ModelA of 202_6 was saved\n",
      "ModelA of 202_4 was saved\n",
      "ModelA of 202_3 was saved\n",
      "ModelA of 202_1 was saved\n",
      "ModelA of 202_8 was saved\n",
      "ModelA of 203_2 was saved\n",
      "ModelA of 203_7 was saved\n",
      "ModelA of 203_5 was saved\n",
      "ModelA of 203_9 was saved\n",
      "ModelA of 203_6 was saved\n",
      "ModelA of 203_4 was saved\n",
      "ModelA of 203_3 was saved\n",
      "ModelA of 203_1 was saved\n",
      "ModelA of 203_8 was saved\n",
      "ModelA of 205_2 was saved\n",
      "ModelA of 205_7 was saved\n",
      "ModelA of 205_5 was saved\n",
      "ModelA of 205_9 was saved\n",
      "ModelA of 205_6 was saved\n",
      "ModelA of 205_4 was saved\n",
      "ModelA of 205_3 was saved\n",
      "ModelA of 205_1 was saved\n",
      "ModelA of 205_8 was saved\n",
      "ModelA of 207_2 was saved\n",
      "ModelA of 207_7 was saved\n",
      "ModelA of 207_5 was saved\n",
      "ModelA of 207_9 was saved\n",
      "ModelA of 207_6 was saved\n",
      "ModelA of 207_4 was saved\n",
      "ModelA of 207_3 was saved\n",
      "ModelA of 207_1 was saved\n",
      "ModelA of 207_8 was saved\n",
      "ModelA of 208_2 was saved\n",
      "ModelA of 208_7 was saved\n",
      "ModelA of 208_5 was saved\n",
      "ModelA of 208_9 was saved\n",
      "ModelA of 208_6 was saved\n",
      "ModelA of 208_4 was saved\n",
      "ModelA of 208_3 was saved\n",
      "ModelA of 208_1 was saved\n",
      "ModelA of 208_8 was saved\n",
      "ModelA of 209_2 was saved\n",
      "ModelA of 209_7 was saved\n",
      "ModelA of 209_5 was saved\n",
      "ModelA of 209_9 was saved\n",
      "ModelA of 209_6 was saved\n",
      "ModelA of 209_4 was saved\n",
      "ModelA of 209_3 was saved\n",
      "ModelA of 209_1 was saved\n",
      "ModelA of 209_8 was saved\n",
      "ModelA of 210_2 was saved\n",
      "ModelA of 210_7 was saved\n",
      "ModelA of 210_5 was saved\n",
      "ModelA of 210_9 was saved\n",
      "ModelA of 210_6 was saved\n",
      "ModelA of 210_4 was saved\n",
      "ModelA of 210_3 was saved\n",
      "ModelA of 210_1 was saved\n",
      "ModelA of 210_8 was saved\n",
      "ModelA of 212_2 was saved\n",
      "ModelA of 212_7 was saved\n",
      "ModelA of 212_5 was saved\n",
      "ModelA of 212_9 was saved\n",
      "ModelA of 212_6 was saved\n",
      "ModelA of 212_4 was saved\n",
      "ModelA of 212_3 was saved\n",
      "ModelA of 212_1 was saved\n",
      "ModelA of 212_8 was saved\n",
      "ModelA of 213_2 was saved\n",
      "ModelA of 213_7 was saved\n",
      "ModelA of 213_5 was saved\n",
      "ModelA of 213_9 was saved\n",
      "ModelA of 213_6 was saved\n",
      "ModelA of 213_4 was saved\n",
      "ModelA of 213_3 was saved\n",
      "ModelA of 213_1 was saved\n",
      "ModelA of 213_8 was saved\n",
      "ModelA of 214_2 was saved\n",
      "ModelA of 214_7 was saved\n",
      "ModelA of 214_5 was saved\n",
      "ModelA of 214_9 was saved\n",
      "ModelA of 214_6 was saved\n",
      "ModelA of 214_4 was saved\n",
      "ModelA of 214_3 was saved\n",
      "ModelA of 214_1 was saved\n",
      "ModelA of 214_8 was saved\n",
      "ModelA of 215_2 was saved\n",
      "ModelA of 215_7 was saved\n",
      "ModelA of 215_5 was saved\n",
      "ModelA of 215_9 was saved\n",
      "ModelA of 215_6 was saved\n",
      "ModelA of 215_4 was saved\n",
      "ModelA of 215_3 was saved\n",
      "ModelA of 215_1 was saved\n",
      "ModelA of 215_8 was saved\n"
     ]
    }
   ],
   "source": [
    "for ID in IDs:\n",
    "    loadTrainVars(ID) #load variables\n",
    "\n",
    "    for run in runs:\n",
    "        # path of saving models and their results\n",
    "        model_path = models_path + 'modelsA_' + str(run) + '_' + str(ret) + '/'\n",
    "        predsPath = '../preds/trainA_outs_' + str(run) + '_' + str(ret) + '/'\n",
    "        buildGraph() #make the graph\n",
    "        runModel(ID) #train the model with train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
